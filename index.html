<!DOCTYPE html>
<!--Adapted from: https://visual.cs.brown.edu/workshops/necv2019/-->
<html lang="en">
    <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!--
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <style type="text/css">
        body {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
            font-weight:300;
            font-size:14px;
            margin-left: auto;
            margin-right: auto;
            width: 90%;
            text-align: justify;
            text-justify: inter-word;
            line-height: 1.5;
        }

        h1 {
            font-size:28px;
            font-weight:250;
        }

        h2 {
            font-size:24px;
        }

        h3 {
            font-size:20px;
        }

        h4 {
            font-size:17px;
        }
        .author_list {
            font-size: 10pt;
        }
        .talk_title {
            font-size: 11pt;
            color: #0B5345;
        }
        
        img.header-img {
            border: 1px solid black;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }
        
        img.rounded {
            border: 1px solid #eeeeee;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }
        
        a:link,a:visited
        {
            color: #006633;
            text-decoration: none;
        }
        a:hover {
            color: #008833;
        }
        
        table { 
            border-collapse: collapse; 
            text-align: left;
        }
        table.table1 td.dl-link {
            height: 140px;
            text-align: center;
            font-size: 15px;
        }
        table.table1 tr {
          border-top: 1pt solid black;
          border-bottom: 1pt solid black;
        }
        
        img.logo {
          display: block;
          margin-left: auto;
          margin-right: auto;
        }
        
        .vert-cent {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }
        
        hr
        {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0.25), rgba(0, 0, 0, 0.25), rgba(0, 0, 0, 0.25));
        }

        .google-maps {
            width: 100%;
            max-width: 100%;
            display: block;
        }
        .google-maps iframe {
            width: 90% !important; 
            height: 250px; 
            display: block; 
        }
        
        @media only screen and (min-width: 768px) { 
            body {
                font-size:20px;
                width: 80%;
            }
                
            h1 {
                font-size:38px;
                font-weight:300;
            }
            h2 {
                font-size:32px;
            }
    
            h3 {
                font-size:28px;
            }
    
            h4 {
                font-size:24px;
            }
            .author_list {
                font-size: 12pt;
            }
            .talk_title {
                font-size: 13pt;
            }    
          	table.table1 td.dl-link {
          		height: 160px;
          		font-size: 22px;
          	}
            table.table1 tr {
              border-top: 1pt solid black;
              border-bottom: 1pt solid black;
            }

            .google-maps iframe {
                height: 400px;
            }
            
        }
  </style>

  <title>NECV 2023</title>
  </head>
    
  <body>
    
    <div class="container w-75" align="center">
        <h1><b>New England Computer Vision Workshop</b></h1>
        <h2><a href="https://home.dartmouth.edu/" target="_blank">Dartmouth College</a>, Hanover, NH</h2>
        <em><h2>Friday, December 1, 2023</h2></em>
    </div>

    <div class="container w-75" align="center">
        <img src="./files/dartmouth.jpg" width="80%" style="margin: 1em 0em 1em 0em; border: 1px solid #000"><br>
    </div>

    <div class="container w-75" >
        <hr>
        <p>
            The New England Computer Vision Workshop (NECV) brings together researchers in computer vision and related areas 
            for an informal exchange of ideas through a full day of presentations and posters. 
            NECV attracts researchers from universities and industry research labs in New England. 
            As in previous years, the workshop will focus on graduate student presentations.
        </p>

        <p>
          Welcome to Dartmouth College!<br><br>
          - <a href="https://souyoungjin.github.io/" target=”_blank”>SouYoung Jin</a>, 
            <a href="https://sites.google.com/view/adithyapediredla/?pli=1" target=”_blank”>Adithya Pediredla</a>, 
            and <a href="https://yuwingtai.github.io/" target=”_blank”>Yu-Wing Tai</a>
        </p>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Registration</h2>

        <h3>Academic Researchers:</h3>
        <p>
            Participation is free for all researchers at academic institutions. 
            To secure your spot and enjoy a guaranteed lunch (for early registrants), 
            please register by the early registration deadline on <u>Monday, November 20, 2023</u>. 
            <br><br>

            <a href="https://forms.gle/3DZmV5CnaJPxWf6K6" target="_blank">
                <b><i>Register Here for Academic Researchers</i></b>
            </a>
        </p>
        
        <h3>Industry Participants:</h3>
        <p>
            For our industry friends, a limited number of registrations are available for a fee. 
            Please contact <a href="mailto:samson@ai.mit.edu" target="_blank">Samson Timoner - samson@ai.mit.edu</a> 
            for registration details.
        <p>        
    </div>

    <hr>
    <div class="container w-75">
        <h2>Submission</font></h2>
        <p>
            Please submit a one-page PDF abstract using the 
            <a href="https://github.com/cvpr-org/author-kit/releases/tag/CVPR2024-v2" target=”_blank”>CVPR 2024 rebuttal template</a> 
            by email to <a href="mailto:necv2023dartmouth@gmail.com">necv2023dartmouth@gmail.com</a>. 
            Please include the title of your work and the list of authors in the abstract. 
            Abstracts are due by <s>Sunday, November 19</s> <b><u>Monday, November 20</u></b>. 
            Oral decisions will be released by November 22.
        </p>

        <p>
            You may present work that has already been published, or work that is in progress. 
            All relevant submissions will be granted a poster presentation, 
            and selected submissions from each institution will be granted 12-minute oral presentations. 
            Post-docs and faculty may submit for poster presentations, but oral presentations are reserved for graduate students.
        </p>

        <p>
            There will be <em>no publications</em> resulting from the workshop, 
            so presentations will not be considered "prior peer-reviewed work" according to any definition we are aware of. 
            Thus, work presented at NECV can be subsequently submitted to other venues without citation.
        </p>

        <p>
            The workshop is after the CVPR submission deadline, so come and show off your new work in a friendly environment. 
            It's also just before the NeurIPS conference, so feel free to come and practice your presentation.
        </p>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Poster Competition</h2>
        We are pleased to announce that we have received a generous donation from NVIDIA.
        As part of their support, the best poster at the workshop will be awarded a GPU. 
        Further details regarding the award will be confirmed and communicated to the recipient at a later date.
        We appreciate NVIDIA's contribution to our workshop and look forward to the exciting competition for the best poster.
        This award is dedicated to graduate student presenters only.
    </div>

    <div class="container w-75">
        <hr>
        <h2>Logistics</h2>
        
        <h3>Tentative Schedule</h3>
        <table class="table1">
            <tbody>
            <tr bgcolor="#E5E8E8">
                <td width=150px height=40px><b>Time</b></td>
                <td width=600px><b>Topic</b></td>
                <td width=200px><b>Location</b></td>
            </tr>
            <tr>
                <td height=40px>08:30-09:30</td>
                <td>Registration, Poster Setup & Breakfast</td>
                <td><i>Cummings 1F</i></td>
            </tr>
            <tr>
                <td height=40px>09:30-09:45</td>
                <td>Welcome & Opening</td>
                <td><i>Cummings 100</i></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>09:45-10:45</td>
                <td><b>Oral Session I</b></td>
                <td><i>Cummings 100</i></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="3">
                    <ul>
                        <li>
                            <span class="talk_title"><b>[16]</b> ISRF: Interactive Segmentation of Radiance Fields.</span> <br>
                            <span class="author_list">Rahul Goel*, <b>Dhawal Sirikonda* (Dartmouth College)</b>, Saurabh Saini, P J Narayanan</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[03]</b> Improving the perception of fiducial markers in the field using Adaptive Active Exposure Control.</span> <br>
                            <span class="author_list"><b>Ziang Ren (Dartmouth College)</b>, Samuel Lensgraf, Alberto Quattrini Li</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[12]</b> Curvature Fields from Shading Fields.</span> <br>
                            <span class="author_list"><b>Xinran Han (Harvard University)</b>, Todd Zickler</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[17]</b> NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation.</span> <br>
                            <span class="author_list"><b>Vikas Thamizharasan (UMass Amherst)</b>, Difan Liu, Matt Fisher, Cherry (N.X.) Zhao, Evangelos Kalogerakis, Mike Lukáč</span>
                        </li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td height=40px>10:45-10:55</td>
                <td>Break</td>
                <td></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>10:55-11:55</td>
                <td><b>Oral Session II</b></td>
                <td><i>Cummings 100</i></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="3">
                    <ul>
                        <li>
                            <span class="talk_title"><b>[34]</b> Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</span> <br>
                            <span class="author_list"><b>Fengyu Yang* (Yale University)</b>, Chao Feng*, Ziyang Chen*, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[33]</b> DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling</span> <br>
                            <span class="author_list"><b>Gustavo Perez (UMass Amherst)</b>, Subhransu Maji, Daniel Sheldon</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[15]</b> GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis</span> <br>
                            <span class="author_list"><b>Yiqing Liang (Brown University)</b>, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[11]</b> AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes</span> <br>
                            <span class="author_list"><b>Rao Fu (Brown University)</b>, Zehao Wen, Zichen Liu, Srinath Sridhar</span>
                        </li>
                    </ul>
                    
                </td>
            </tr>
            <tr>
                <td height=40px>12:00-13:00</td>
                <td>Lunch</td>
                <td><i>ECSC 116</i></td>
                <td></td>
            </tr>
            <tr bgcolor="#EBF5FB">
                <td height=40px>13:00-14:30</td>
                <td><b>Poster Session</b></td>
                <td><i>ECSC 1F</i></td>
            </tr>
            <tr bgcolor="#EBF5FB">
                <td colspan="3">
                    <ul>
                        <li>
                            <span class="talk_title"><b>[01]</b> Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</span> <br>
                            <span class="author_list">William Shen*, <b>Ge Yang* (MIT)</b>, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, Phillip Isola</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[02]</b> Language-Driven Appearance and Physics Editing via Feature Splatting</span> <br>
                            <span class="author_list">Rizhao Qiu*, <b>Ge Yang* (MIT)</b>, Weijia Zeng, Xiaolong Wang</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[05]</b> Snapshot Lidar: Fourier embedding of amplitude and phase for single-image depth reconstruction</span> <br>
                            <span class="author_list"><b>Sarah Friday (Dartmouth College)</b>, Yunzi Shi, Yaswanth Kumar Cherivirala, Vishwanath Saragadam, Adithya Pediredla</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[06]</b> The GAN is dead; long live the GAN!</span> <br>
                            <span class="author_list"><b>Nick Huang (Brown University)</b>, Aaron Gokaslan, James Tompkin</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[07]</b> Underwater Camera Calibration: N-Sphere Camera Model and Extensions</span> <br>
                            <span class="author_list"><b>Monika Roznere (Dartmouth College)</b>, Adithya K. Pediredla, Samuel E. Lensgraf, Yogesh Girdhar, and Alberto Quattrini Li</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[08]</b> On Human-like Biases in Convolutional Neural Networks for the Perception of Slant from Texture</span> <br>
                            <span class="author_list">Yuanhao Wang, <b>Qian Zhang (Brown University)</b>, Celine Aubuchon, Jovan Kemp, Fulvio Domini, James Tompkin</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[09]</b> Toward Physically-based 360° Intrinsic Decomposition from RGBD Images</span> <br>
                            <span class="author_list"><b>Qian Zhang (Brown University)</b>, James Tompkin</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[13]</b> Direct Superpoints Matching for Robust Point Cloud Registration</span> <br>
                            <span class="author_list"><b>Aniket Gupta (Northeastern University)</b>, Yiming Xie, Hanumant Singh, Huaizu Jiang</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[14]</b> FT2TF: First-Person Statement Text-To-Talking Face Generation</span> <br>
                            <span class="author_list"><b>Xingjian Diao (Dartmouth College)</b>, Ming Cheng, Wayner Barrios, SouYoung Jin</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[18]</b> OmniControl: Control Any Joint at Any Time for Human Motion Generation</span> <br>
                            <span class="author_list">Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, <b>Huaizu Jiang (Northeastern University)</b></span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[19]</b> PlatoNeRF: 3D Reconstruction in Plato’s Cave via Single-View Two-Bounce Lidar</span> <br>
                            <span class="author_list"><b>Tzofi Klinghoffer (MIT)</b>, Xiaoyu Xiang*, Siddharth Somasundaram*, Yuchen Fan, Christian Richardt, Ramesh Raskar, Rakesh Ranjan</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[20]</b> Preserving Tumor Volumes for Unsupervised Medical Image Registration</span> <br>
                            <span class="author_list"><b>Qihua Dong (Northeastern University)</b>, Hao Du, Ying Song, Yan Xu, Jing Liao</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[21]</b> SPASM: Small PArallax Structure from Motion</span> <br>
                            <span class="author_list"><b>Fabien Delattre (UMass Amherst)</b>, David Dirnfeld, Zhipeng Tang, Pedro Miraldo, Erik Learned-Miller</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[22]</b> SurfsUp: learning fluid simulation for novel surfaces</span> <br>
                            <span class="author_list">Arjun Mani*, <b>Ishaan Chandratreya* (MIT)</b>, Elliot Creager, Carl Vondrick, Richard Zemel</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[23]</b> Toward Perceptually-guided Environment-adaptive AR Visualization</span> <br>
                            <span class="author_list"><b>Hojung (Ashley) Kwon (Brown University)</b>, Yuanbo Li, Xiaohan (Chloe) Ye, Praccho Muna-McQuay, Liuren Yin, James Tompkin</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[25]</b> ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape Completion</span> <br>
                            <span class="author_list"><b>Hongyu Li (Brown University)</b>, Snehal Dikhale, Soshi Iba, Nawid Jamali</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[26]</b> Vision Beyond Borders: Transforming Single-View inputs into Multi-View vision</span> <br>
                            <span class="author_list"><b>Mingyuan Zhang (Northeastern University)</b>, Chang Liu, Yue Bai, Yun Fu</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[27]</b> TASK2BOX: Box Embeddings for Modeling Asymmetric Task Relationships</span> <br>
                            <span class="author_list"><b>Rangel Daroya (UMass Amherst)</b>, Aaron Sun, Subhransu Maji</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[28]</b> Self-supervised Learning using Hypercube Embeddings</span> <br>
                            <span class="author_list"><b>Deep Chakraborty (UMass Amherst)</b>, Erik Learned-Miller</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[29]</b> Rewrite the Stars</span> <br>
                            <span class="author_list"><b>Xu Ma (Northeastern University)</b>,  Xiyang Dai, Yue Bai, Yizhou Wang, Yun Fu</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[30]</b> Latent Graph Inference with Limited Supervision</span> <br>
                            <span class="author_list"><b>Jianglin Lu (Northeastern University)</b>, Yi Xu, Huan Wang, Yue Bai, Yun Fu</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[32]</b> FLARE: Film Language and Audiovisual Representation Engine for Movie Audio Description</span> <br>
                            <span class="author_list"><b>Wayner Barrios (Dartmouth College)</b>, Henry Scheible, SouYoung Jin</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[35]</b> A Unified Framework for Domain Adaptive Object Detection</span> <br>
                            <span class="author_list"><b>Justin Kay (MIT)</b>, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young, Pietro Perona, Sara Beery, Grant Van Horn</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[36]</b> Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions</span> <br>
                            <span class="author_list">Oindrila Saha, Grant Van Horn, <b>Subhransu Maji (UMass Amherst)</b></span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[37]</b> Is CLIP Fooled by Optical Illusions?</span> <br>
                            <span class="author_list"><b>Jerry Ngo (MIT)</b>, Swami Sankaranarayanan, Phillip Isola</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[38]</b> Frame Flexible Network</span> <br>
                            <span class="author_list"><b>Yitian Zhang (Northeastern University)</b>, Yue Bai, Chang Liu, Huan Wang, Sheng Li, Yun Fu</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[39]</b> See Beyond Vision: Layout Trajectory Sequence Prediction From Noisy Mobile Modality</span> <br>
                            <span class="author_list"><b>Haichao Zhang (Northeastern University)</b></span>
                        </li>
                    </ul>
                    
                </td>
            </tr>
            <tr>
                <td height=40px>14:30-14:50</td>
                <td>Coffee Break</td>
                <td><i>Cummings 1F</i></span></td>
                <td></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>14:50-15:50</td>
                <td><b>Oral Session III</b></td>
                <td><i>Cummings 100</i></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="3">
                    <ul>
                        <li>
                            <span class="talk_title"><b>[10]</b> 3D Reconstruction of Occluded and Specular Objects using Multi-Bounce Lidar</span> <br>
                            <span class="author_list"><b>Siddharth Somasundaram (MIT)</b>, Connor Henley, Akshat Dave, Joseph Hollmann, Ashok Veeraraghavan, Ramesh Raskar</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[24]</b> Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction</span> <br>
                            <span class="author_list"><b>Yi Xu (Northeastern University)</b>, Armin Bazarjani, Hyung-gun Chi, Chiho Choi, Yun Fu</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[31]</b> Inferring the Future by Imagining the Past</span> <br>
                            <span class="author_list"><b>Kartik Chandra* (MIT)</b>, <b>Tony Chen* (MIT)</b>, Tzu-Mao Li, Jonathan Ragan-Kelley, Joshua Tenenbaum</span>
                        </li>
                        <li>
                            <span class="talk_title"><b>[04]</b> Multi-Irreducible Spectral Synchronization for Robust Rotation Averaging</span> <br>
                            <span class="author_list"><b>Owen Howell (Northeastern University)</b>, Haoen Huang and David Rosen</span>
                        </li>
                    </ul>
                    
                </td>
            </tr>
            <tr>
                <td height=40px>15:50-16:00</td>
                <td>Closing Remark</td>
                <td><i>Cummings 100</i></span></td>
            </tr> 
            </tbody>
        </table>

        <h3>Venue</h3> 
        Our workshop will be held at Dartmouth College.
        For a detailed campus map and directions, please refer to 
        <a href="https://home.dartmouth.edu/about/campus-map" target=”_blank”>our Campus Map</a>.
        
        <p class="google-maps">
            <iframe 
                src="https://www.google.com/maps/d/u/0/embed?mid=1ZVQWZtSJ6lzEl-1ZEKp0CxHv1n4ZGG8&ehbc=2E312F&noprof=1" 
            ></iframe>
        </p>
        <p>
            <b>Cummings</b> 
            <i class="fa fa-heart"></i>
            <ul>
                <li>10 Engineering Dr, Hanover, NH 03755 
                    (<a href="https://maps.app.goo.gl/TBV7ZDGdiuK9gyuj9" target=”_blank”>Google Maps</a>)
                </li>
                <li>Oral Session, Coffee Break, Opening/Closing, Morning Registration</li>
            </ul>
        </p>
        <p>
            <b>ECSC</b>
            <i class="fa fa-star"></i>
            <ul>
                <li>15 Engineering Dr, Hanover, NH 03755 
                    (<a href="https://maps.app.goo.gl/zCSNvXv6kZ8RWD919" target=”_blank”>Google Maps</a>)
                </li>
                <li>Poster Session, Lunch, Afternoon Registration</li>
            </ul>
        </p>
        
        <h3>Getting here:</h3>
        <p>
            <ul>
                <li><b>By Coach:</b> 
                    If you plan to arrive by coach, 
                    there is <a href="https://dartmouthcoach.com/" target=”_blank>Dartmouth Coach</a> that offers scheduled trips 
                    between the Dartmouth campus, Boston's South Station, and Logan Airport. 
                    Dartmouth Coach also offers services between Hanover and midtown Manhattan. 
                    Please find the map above showing how to walk from the Dartmouth Coach stop to the workshop venue. 
                </li>
                <li><b>By Car:</b> 
                    <b>Free parking</b> is available in the Emily and Errik Anderson Parking Garage for attendees arriving by car. 
                    Please refer to <a href="https://necv2023.github.io/files/NECV%20Parking%20instructions.pdf" target="_blank"><b>this instruction</b></a> 
                    to obtain a parking ticket and register it at the designated registration desk. 

                    <p>
                        <b>Emily and Errik Anderson Parking Garage</b>
                        <i class="fa-solid fa-square-parking"></i>
                        <ul>
                            <li>15 Engineering Dr, Hanover, NH 03755 
                                (<a href="https://maps.app.goo.gl/svwY2dmNKRNBeb6f7" target=”_blank”>Google Maps</a>)
                            </li>
                            <li>Please DO NOT insert your credit card into the parking machine.</li>
                        </ul>
                    </p>
                    
                    There are also some ParkMobile spaces on campus and all-day spaces in the G lot. 
                    Please check <a href="https://www.dartmouth.edu/transportation/parking/cost_information/visitors.php" target="_blank">this link</a> 
                    for more information on visitor parking.
                </li>
            </ul>
        </p>
        
        <h3>WiFi</h3>
        <p>
            Log on to "Dartmouth Public" and accept the terms. 
            Alternatively, if your home institution participates in eduroam, then you can also use eduroam 
            to connect wirelessly while on our campus.
        </p>
        
        <!--
        <h3>Hotel Accommodation</h3>
        <p>
            If you plan to come a day ahead or want to stay an extra day, we recommend booking your room early to ensure comfortable lodging. 
            Workshop participants can take advantage of a special rate at 
            <a href="https://www.hanoverinn.com/" target="_blank"> Hanover Inn</a>, conveniently located right next to the campus.
        </p>
        
        <p>
            <ul>
                <li>Special Workshop Rate: $220.00 (+tax) per room per night</li>
                <li>Available Nights: November 30 and December 1</li>
            </ul>
        <p>
            To secure this special rate, please call <a href="https://www.hanoverinn.com/" target="_blank">Hanover Inn</a> at 603-643-4300 and 
            identify yourself to the reservations agent as a participant of the NECV workshop. 
            The discounted rate is available for 10 rooms per night, and the hotel will honor this rate until <u>November 10</u>. 
            We recommend booking early to ensure availability and enjoy the Upper Valley for an extended stay!
        </p>
        -->
            
    </div>

    <div class="container w-75">
        <hr>
        <h2>Sponsorship</font></h2>
        We are grateful to <a href="https://home.dartmouth.edu/" target="_blank">Dartmouth College</a> for providing the venue. 
        We also thank 
        <a href="https://web.cs.dartmouth.edu/" target="_blank">Dartmouth CS</a>, 
        <a href="https://neukom.dartmouth.edu/" target="_blank">Neukom Institute</a>, 
        <a href="" target="_blank">Dartmouth Science Division</a>, and
        <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA</a> for their sponsorship of this event.<br>

        <p>
            <a href="https://web.cs.dartmouth.edu/" target="_blank">
                <img class="logo" src="./files/DartmouthCS_logo.png" width="70%">
            </a>
        </p>
        <p>
            <a href="https://neukom.dartmouth.edu/" target="_blank">
                <img class="logo" src="./files/NeukomInstitute_logo.png" width="70%">
            </a>
        </p>
        <p>
            <a href="https://www.nvidia.com/en-us/research/" target="_blank">
                <img class="logo" src="./files/nvidia_logo.png" width="70%">
            </a>
        </p>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Organizing Committee</h2>
            <p>
                <h3>Workshop Chair</h3>
                <a href="https://souyoungjin.github.io/" target="_blank">SouYoung Jin</a>, 
                <a href="https://sites.google.com/view/adithyapediredla/?pli=1" target="_blank">Adithya Pediredla</a>, 
                <a href="https://yuwingtai.github.io/" target="_blank">Yu-Wing Tai</a>
            </p>
            <p>
                <h3>Corporate Relations Chair</h3>
                <a href="https://people.csail.mit.edu/samson/" target="_blank">Samson Timoner</a>
            </p>
            <p>
                <h3>Logistic Chair</h3>
                <a href="https://web.cs.dartmouth.edu/people/julia-ganson" target="_blank">Julia D. Ganson</a>
            </p>
            <p>
                <h3>Photographer</h3>
                <a href="https://klenhart.us/" target="_blank">Katherine M. Lenhart</a>
            </p>
            <p>
                <h3>Student Volunteer</h3>
                <a href="https://dhawal1939.github.io/" target="_blank">Dhawal Sirikonda</a>, 
                <a href="https://penguuuin.github.io/wenjun.github.io/" target="_blank">Wenjun Liu</a>,
                <a href="https://waybarrios.com/" target="_blank">Wayner Barrios</a>, 
                <a href="https://xid32.github.io/" target="_blank">Xingjian Diao</a>,
                <a href="https://www.linkedin.com/in/sarah-k-friday" target="_blank">Sarah Friday</a>
                
            </p>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Acknowledgements</font></h2>
        <p>
            Thank you to Susan Cable and Andrila Hait Chakrabarti for helping us arrange NECV 2023. 
            Thank you also to the steering committee: 
            Phillip Isola, Pulkit Agrawal, 
            James Tompkin, Benjamin Kimia, 
            Todd Zickler, 
            Yun Raymond Fu, Octavia Camps, 
            Kate Saenko, Margrit Betke, Sclaroff Stan, 
            Erik Learned-Miller, and Subhransu Maji.
        </p>
        <h3>Past Years</h3>
        <ul>
            <li>2022 - MIT <a href="https://necv2022.github.io/" target=”_blank>(website)</a></li>
            <li>2019 - Brown University <a href="https://visual.cs.brown.edu/workshops/necv2019/" target=”_blank>(website)</a></li>
            <li>2018 - Harvard University <a href="https://projects.iq.harvard.edu/necv2018/" target=”_blank>(website)</a></li>
            <li>2017 - Northeastern University <a href="https://web.northeastern.edu/smilelab/necv2017/index.html" target=”_blank>(website)</a></li>
            <li>2016 - Boston University <a href="http://vision.cs.uml.edu/necv2016.html" target=”_blank>(website)</a></li>
            <li>2015 - UMass Amherst <a href="https://people.cs.umass.edu/~smaji/nevm2015/" target=”_blank>(website)</a></li>
        </ul>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <!--<script src="./NECV 2019_files/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="./NECV 2019_files/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="./NECV 2019_files/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>-->
  
</body></html>
